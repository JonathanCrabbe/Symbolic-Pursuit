{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Synthetic experiments with Symbolic Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we shall reproduce one of the experiments from Section 6.1 of the paper.\n",
    "The idea is to start with a linear pseudo black-box for which the importance vector is known unambiguously and see which interpretability methods identifies this vector the most precisely. Let us start by the useful imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symbolic_pursuit.models import SymbolicRegressor  # our symbolic model class\n",
    "from sklearn.metrics import mean_squared_error # we are going to assess the quality of the model based on the generalization MSE\n",
    "from sympy import init_printing # We use sympy to display mathematical expresssions \n",
    "import numpy as np # we use numpy to deal with arrays\n",
    "import lime \n",
    "import lime.lime_tabular\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a linear pseudo black-box $f$ defined on a 3 dimensional feature space.\n",
    "\n",
    "$$ f(x_1,x_2,x_3)= x_1 + 2 \\cdot x_2 + 3 \\cdot x_3$$ \n",
    "\n",
    "The importance vector associated to this model is trivially given by $\\beta = (1,2,3)$ In this case, we shall keep it unnormalised, unlike in the main paper as we deal with few examples. Let us translate this in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return X[:, 0]+2*X[:,1]+3*X[:,2]\n",
    "\n",
    "dim_X = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now draw uniformly 100 test points  that we will feed to a *LIME* explainer <cite data-cite=\"2480681/WCEBQ7N9\"></cite> and to train a Symbolic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts = 100\n",
    "X = np.random.uniform(0, 1, (n_pts, dim_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we draw 10 test ponits $x_{test} \\equiv U([0,1]^3)$ that we are going to use in order to evaluate the perfomances of both explainers on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10\n",
    "X_test = np.random.uniform(0, 1, (n_test, dim_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LIME produces importance vectors with entries in the form $(feature \\ domain , importance)$ for each feature appearing in decreasing order of importance, we implement a function which identifies the feature from the first entry of the tuple and who sorts the importances in the form $(importance(x_1), importance(x_2), importance(x_3))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_weights(exp_list):\n",
    "    ordered_weights = [0 for _ in range(dim_X)]\n",
    "    for tup in exp_list:\n",
    "        feature_id = int(tup[0].split('x_')[1][0])\n",
    "        ordered_weights[feature_id-1] = tup[1]    \n",
    "    return ordered_weights    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract the feature importance for our 10 test points as predicted by the LIME explainer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_weight_list = []\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X, \n",
    "                                                   feature_names=[\"x_\"+str(k) for k in range(1,dim_X+1)], \n",
    "                                                   class_names=['f'], \n",
    "                                                   verbose=True,\n",
    "                                                   mode='regression')\n",
    "\n",
    "for i in range(n_test):\n",
    "    exp = explainer.explain_instance(X_test[i], f, num_features=dim_X)\n",
    "    lime_weight_list.append(order_weights(exp.as_list()))  \n",
    "                            \n",
    "print(lime_weight_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the last output, which is the list of predicted importance vectors, LIME seems to produce a big variety of importance vectors. This is suprising for a global linear model. We also note that the relative importance seem inconsistent with the true importance vector $\\beta$ defined above. Let us now train a Symbolic model for $f$ based on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_model = SymbolicRegressor()\n",
    "symbolic_model.fit(f, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now ask our symbolic model to predict the importance vectors for each test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_weight_list = [] \n",
    "for k in range(n_test):\n",
    "    symbolic_weight_list.append(symbolic_model.get_feature_importance(X_test[k]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(symbolic_weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our results appear to be always consistent and very close to the true importance vector $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2480681/WCEBQ7N9": {
     "URL": "http://arxiv.org/abs/1602.04938",
     "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
     "accessed": {
      "day": 29,
      "month": 4,
      "year": 2020
     },
     "author": [
      {
       "family": "Ribeiro",
       "given": "Marco Tulio"
      },
      {
       "family": "Singh",
       "given": "Sameer"
      },
      {
       "family": "Guestrin",
       "given": "Carlos"
      }
     ],
     "container-title": "arXiv:1602.04938 [cs, stat]",
     "id": "2480681/WCEBQ7N9",
     "issued": {
      "day": 9,
      "month": 8,
      "year": 2016
     },
     "note": "arXiv: 1602.04938",
     "shortTitle": "\"Why Should I Trust You?",
     "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
     "title-short": "\"Why Should I Trust You?",
     "type": "article-journal"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
