{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Symbolic Pursuit vs LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we build a black-box MLP model for the UCI *wine quality* dataset *(Cortez et al. 2009)*.\n",
    "We then build a symbolic model for this black-box and we compare our model to a *LIME* explainer *(Ribeiro, Singh, and Guestrin 2016)*. Let us begin with the useful imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symbolic_pursuit.models import SymbolicRegressor\n",
    "from experiments.train_model import train_model\n",
    "from datasets.data_loader_UCI import mixup, data_loader\n",
    "from sympy.printing.latex import latex\n",
    "import sklearn\n",
    "import lime \n",
    "import lime.lime_tabular\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we split the dataset into a training and a test set. We use the mixup technique to produce a training set for the faithful model *(Zhang et al. 2018)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = data_loader(dataset_name='wine-quality-red')\n",
    "X_random = mixup(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the MLP blackbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train, y_train, black_box=\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the symbolic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_model = SymbolicRegressor()\n",
    "symbolic_model.fit(model.predict, X_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a *LIME* explainer for this regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, \n",
    "                                                   feature_names=[\"X\"+str(k) for k in range(len(X_train[0]))], \n",
    "                                                   class_names=['quality'], \n",
    "                                                   verbose=True,\n",
    "                                                   mode='regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask to the LIME interpeter an explanation for a prediction in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "exp = explainer.explain_instance(X_test[i], model.predict, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like $X_{10}$ is the most important feature for this prediction. A linear expansion of our symbolic model around this test point should give a similar result. Let us produce a first order Taylor expansion of our faithful model around this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly1 = symbolic_model.get_taylor(X_test[i], approx_order=1)\n",
    "poly1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily get a $\\LaTeX$ expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex(poly1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that, indeed, $X_{10}$ has the highest weight in this local linear model. Also note that, as suggested by *LIME*, $X_9$ also has an important weight in this polynomial. However, the agreement is not perfect since $X_2$ has a small weight in our local model. One big advantage of our *Symbolic Pursuit* scheme is that we can actually choose the order of the local model obtained by performing a Taylor expansion in a post-hoc analysis. For instance, we can get an idea of the interactions by going one order above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2 = symbolic_model.get_taylor(X_test[i], approx_order=2)\n",
    "poly2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extract the $\\LaTeX$ expression easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex(poly2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can develop these square to have a less compact expression where we can look at the individual interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import expand\n",
    "expand(poly2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By having a quick look at this expression, we note that the most significant interaction is between $X_{6}$ and $X_{10}$ followed closely by the interaction between $X_{1}$ and $X_{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cortez, Paulo, António Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. 2009. “Modeling Wine Preferences by Data Mining from Physicochemical Properties.” Decision Support Systems 47 (4): 547–53. doi:10.1016/j.dss.2009.05.016.\n",
    "\n",
    "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier.” arXiv:1602.04938 [cs, Stat], August. http://arxiv.org/abs/1602.04938.\n",
    "\n",
    "Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2018. “Mixup: Beyond Empirical Risk Minimization.” arXiv:1710.09412 [cs, Stat], April. http://arxiv.org/abs/1710.09412."
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2480681/H82VI2CA": {
     "URL": "http://arxiv.org/abs/1710.09412",
     "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
     "accessed": {
      "day": 16,
      "month": 4,
      "year": 2020
     },
     "author": [
      {
       "family": "Zhang",
       "given": "Hongyi"
      },
      {
       "family": "Cisse",
       "given": "Moustapha"
      },
      {
       "family": "Dauphin",
       "given": "Yann N."
      },
      {
       "family": "Lopez-Paz",
       "given": "David"
      }
     ],
     "container-title": "arXiv:1710.09412 [cs, stat]",
     "id": "2480681/H82VI2CA",
     "issued": {
      "day": 27,
      "month": 4,
      "year": 2018
     },
     "note": "arXiv: 1710.09412",
     "shortTitle": "mixup",
     "title": "mixup: Beyond Empirical Risk Minimization",
     "title-short": "mixup",
     "type": "article-journal"
    },
    "2480681/W63LCK3U": {
     "DOI": "10.1016/j.dss.2009.05.016",
     "URL": "https://linkinghub.elsevier.com/retrieve/pii/S0167923609001377",
     "accessed": {
      "day": 3,
      "month": 4,
      "year": 2020
     },
     "author": [
      {
       "family": "Cortez",
       "given": "Paulo"
      },
      {
       "family": "Cerdeira",
       "given": "António"
      },
      {
       "family": "Almeida",
       "given": "Fernando"
      },
      {
       "family": "Matos",
       "given": "Telmo"
      },
      {
       "family": "Reis",
       "given": "José"
      }
     ],
     "container-title": "Decision Support Systems",
     "container-title-short": "Decision Support Systems",
     "id": "2480681/W63LCK3U",
     "issue": "4",
     "issued": {
      "year": 2009
     },
     "journalAbbreviation": "Decision Support Systems",
     "language": "en",
     "page": "547-553",
     "page-first": "547",
     "title": "Modeling wine preferences by data mining from physicochemical properties",
     "type": "article-journal",
     "volume": "47"
    },
    "2480681/WCEBQ7N9": {
     "URL": "http://arxiv.org/abs/1602.04938",
     "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
     "accessed": {
      "day": 29,
      "month": 4,
      "year": 2020
     },
     "author": [
      {
       "family": "Ribeiro",
       "given": "Marco Tulio"
      },
      {
       "family": "Singh",
       "given": "Sameer"
      },
      {
       "family": "Guestrin",
       "given": "Carlos"
      }
     ],
     "container-title": "arXiv:1602.04938 [cs, stat]",
     "id": "2480681/WCEBQ7N9",
     "issued": {
      "day": 9,
      "month": 8,
      "year": 2016
     },
     "note": "arXiv: 1602.04938",
     "shortTitle": "\"Why Should I Trust You?",
     "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
     "title-short": "\"Why Should I Trust You?",
     "type": "article-journal"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
